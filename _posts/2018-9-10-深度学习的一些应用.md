---
layout: post
---

# 深度学习有趣的应用

## 短材料

- 用深度学习帮助辨别农业[植物病虫害](https://www.youtube.com/watch?v=NlpS-DhayQA&index=3&list=PLQY2H8rRoyvxjVx3zfw4vA4cvlKogyLNN)
- 格林深瞳的[视觉应用](https://youtu.be/xhp47v5OBXQ)包括:车辆型号识别,人脸视频,运动轨迹检测,人体检测识别,运动密度识别,第一次看的时候,特别震撼.特别的,这而且是一家国内的公司

* 深度学习googletensorflow在2017年做的官方宣传,里面有深度学习[大量的应用](https://www.youtube.com/watch?v=mWl45NkFBOc&list=PLOU2XLYxmsIKGc_NBoIhTn2Qhraji53cv)实例
* YOLO物体识别,在[电影中的应用](https://youtu.be/VOC3huqHrss)
* [肢体动作结构](https://medium.com/tensorflow/move-mirror-an-ai-experiment-with-pose-estimation-in-the-browser-using-tensorflow-js-2f7b769f9b23),在真正的街舞训练,或者动作矫正的时候,很有用

## 演讲,书和讲座

* YOLO发明者介绍物体识别的发展流程[ted演讲](https://www.ted.com/talks/joseph_redmon_how_a_computer_learns_to_recognize_objects_instantly)
* 可口可乐公司用摄像头识别瓶盖中的[兑奖码](https://www.youtube.com/watch?v=WIV6oUc2JOg&index=20&list=PLQY2H8rRoyvxjVx3zfw4vA4cvlKogyLNN)
* 给[视频配音](https://youtu.be/0FW99AQmMc8),案例中,使用的是用棍子敲打桌面,和草地
* 语音[合成音乐](http://www.dtic.upf.edu/~mblaauw/NPSS/),有时候能让你分辨不出来,竟然是合成的
* [deepdream](https://youtu.be/SCE-QeDfXtA)一种让你迷失到起鸡皮疙瘩的有趣视频
* 你可以在台湾大学的这个[演讲视频](https://www.youtube.com/watch?v=X-Q72NiI3SQ&t=2s)(1:24:19)中找到一些应用
* 总结的深度学习的30个[应用案例](http://www.yaronhadad.com/deep-learning-most-amazing-applications/#wild)
* 这个视频,你能了解深度[学习的硬件TPU](https://www.youtube.com/watch?v=zEOtG-ChmZE)
* 大数据在各行业的应用的[书](https://item.jd.com/12023669.html),书中有提到政务,学校,商业,个人中的案例使用

## 为什么梯度需要用迭代法计算很多次

* 梯度下降法[维基百科](https://www.wikiwand.com/zh-hans/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95)
* 迭代次数,是一个超参数,把**整个数据集迭代一次,相当与一次梯度下降**,在计算最小参数的时候,需要多次梯度下降,所以当然需要多个`epochs`.
* 为什么需要多个 minibatch 呢?由于数据集有可能太大了,以致于无法直接放入内存,所以要把一个数据集,拆分为多个 mini batch
* 你也可以从[Quora](https://www.quora.com/What-is-an-epoch-in-deep-learning)中找到相应的解释,摘抄的内容在下面

>In Deep Learning, an **epoch is a hyperparameter** which is defined before training a model. One epoch is when an **entire dataset is passed both forward and backward through the neural network only once**.
>
>One epoch is too big to feed to the computer at once. So, we divide it in several smaller batches. We use more than one epoch because passing the entire dataset through a neural network is not enough and we need to pass the full dataset multiple times to the same neural network. But since we are using a limited dataset and to optimise the learning and the graph we are using **Gradient Descent** which is an iterative process. So, updating the weights with single pass or one epoch is not enough.
>
>A **batch is the total number of training examples present in a single batch and an iteration is the number of batches needed to complete one epoch**.
>
>For example: If we divide a dataset of 2000 training examples into 500 batches, then 4 iterations will complete 1 epoch.

## 为什么ReLU函数管用

* 线性函数+线性函数=线性函数
* 非线性函数+非线性函数=非线性函数
* 线性函数+非线性函数=非线性函数
* [medium参考链接](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)表明,任何函数,都可以用ReLU来近似,你可以在这个[视频](https://www.youtube.com/watch?v=Ijqkc7OLenI)中了解到一个局部,结合[这个](https://www.youtube.com/watch?v=9Q5GrXr9fZg)更容易理解,视频中表明,最简单的Step Function都能近似任何曲线,当更多的隐藏层,更能近似的替代了
* 决策树上每个节点,相当于深度学习中对一个属性上,做的一次线性分类器